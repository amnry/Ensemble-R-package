
---
title: "Vignette for simpleEnsembleGroup22 Package"
author: "Sravya Yarlagadda, Hasan Md Moonam, Aman Arya, Eric Seeram"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Introduction

This vignette describes the usage and internal workings of the `simpleEnsembleGroup22` package, which includes various machine learning techniques for regression and ensemble learning.

```{r setup, include=FALSE, results='hide', message=FALSE, echo=FALSE}
options(max.print = 5) 
options(repos = "https://cran.r-project.org")

```
## Importing the Package

To use the `simpleEnsembleGroup22` package, you will have to install the package like this - 

```{r installing package}

install.packages("simpleEnsembleGroup22_0.0.0.9000.tar")
library("simpleEnsembleGroup22")

```
## Internal Workings of Various Functions and Examples

### 1. Linear Regression Model (`linear_model`)

The linear_model function fits a linear model using either linear regression for continuous response variables or logistic regression for binary response variables.

Description:
This function accepts two arguments:

X: Matrix of predictors . Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.
y: Response variable. For binary outcomes, it should be a binary vector (0 or 1). For continuous outcomes, it should be a numeric vector.
It returns a list containing the fitted model and predicted values.

Details:
For continuous response variables, linear_model performs linear regression.
For binary response variables, it performs logistic regression.

Mathematics:
Linear regression fits a model to minimize the sum of squared residuals. This means it finds the line (or hyperplane in higher dimensions) that best fits the data by minimizing the difference between the observed and predicted values.

Export:
This function is exported (@export), making it accessible to users of the package.

**Examples**:

```{r linear-example, message=FALSE, collapse=TRUE, max.lines=5}

library(MASS)
data(Boston)
str(Boston) 
X <- Boston[, 2:14]
y <- Boston[,1]
linear_results <- linear_model(X, y) #For Continuous Response variable
linear_results

data(Pima.te)  
str(Pima.te)
X <- Pima.te[, 1:7]
y <- Pima.te[,8]
logistic_results <- linear_model(X, as.numeric(y) - 1) #For Binary Response variable
logistic_results

```

### 2. Ridge Regression (`ridge_model`)

The ridge_model function performs ridge regression, which is a regularization technique used to mitigate multicollinearity in the data and prevent overfitting.

Ridge regression adds a penalty term to the standard linear regression objective function, minimizing the sum of squared residuals plus a penalty term that shrinks the coefficients towards zero.
The function performs cross-validation to select the optimal lambda value, which controls the amount of regularization applied.We have used 10-fold cross-validation as the standard here.
It fits the final ridge regression model using the optimal lambda value.

Description:
This function accepts two arguments:

X: Matrix of predictors . Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.
y: Response variable. For binary outcomes, it should be a binary vector (0 or 1). For continuous outcomes, it should be a numeric vector.

Value:
The function returns a list containing two elements:

model: Fitted ridge regression model.
predicted.value: Predicted values based on the fitted model.

Export:
This function is exported (@export), making it accessible to users of the package.

**Example**:

```{r ridge-example}
library(MASS)
data(Boston)
str(Boston) 
X <- Boston[, 2:14]
y <- Boston[,1]
ridge_results <- ridge_model(X, y) #For Continuous Response variable
ridge_results

data(Pima.te) 
str(Pima.te)
X <- Pima.te[, 1:7]
y <- Pima.te[,8]
ridge_results <- ridge_model(X, as.numeric(y) - 1) #For Binary Response variable
ridge_results
```

### 3. Lasso Regression (`lasso_model`)

The lasso_model function fits a Lasso regression model using the glmnet package with cross-validation to select the optimal lambda value. We have used 10-fold cross-validation as the standard here.

Description:
This function accepts three arguments:

X: Matrix of predictors . Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.
y: Response variable. For binary outcomes, it should be a binary vector (0 or 1). For continuous outcomes, it should be a numeric vector.

It returns a list containing the fitted model and predicted values.

**Example**:

```{r lasso-example}
library(MASS)
data(Boston)
str(Boston) 
X <- Boston[, 2:14]
y <- Boston[,1]
lasso_results <- lasso_model(X, y) #For Continuous Response variable
lasso_results

data(Pima.te) 
str(Pima.te)
X <- Pima.te[, 1:7]
y <- Pima.te[,8]
lasso_results <- lasso_model(X, as.numeric(y) - 1) #For Binary Response variable
lasso_results
```
### 4. Elastic Net Regression (`elastic_net_regression`)

The elastic_net_regression function fits an elastic net regression model using the glmnet package with cross-validated alpha selection. This function automatically detects the type of response variable (binary or continuous) and adjusts the family parameter in the glmnet function call accordingly.

Description:
This function accepts two arguments:

X: Matrix of predictors . Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.
y: Response variable. For binary outcomes, it should be a binary vector (0 or 1). For continuous outcomes, it should be a numeric vector.

It returns a list containing three elements:

model: Fitted elastic net regression model object.
optimal.alpha: Optimal alpha value selected through cross-validation.
predicted.value: Predicted values based on the fitted model.

**Example**:

```{r elastic-net-example}
library(MASS)
data(Boston)
str(Boston) 
X <- Boston[, 2:14]
y <- Boston[,1]
elastic_net_results <- elastic_net_regression(X, y) #For Continuous Response variable
elastic_net_results

data(Pima.te) 
str(Pima.te)
X <- Pima.te[, 1:7]
y <- Pima.te[,8]
elastic_net_results <- elastic_net_regression(X, as.numeric(y) - 1) #For Binary Response variable
elastic_net_results
```

### 5. Random Forest (`rf_model`)

A Random Forest is machine learning model that combines the predictions of multiple decision trees to improve the overall predictive accuracy and reduce overfitting.

The number of trees (ntree) and the number of variables randomly sampled as candidates at each split (mtry) are set to default values. For classification tasks, mtry is set to the square root of the number of predictors (sqrt(ncol(X))), and for regression tasks, it is set to one-third of the number of predictors (ncol(X) / 3). These are default values.

Description:
This function accepts two arguments:

X: Matrix of predictors . Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.
y: Response variable. For binary outcomes, it should be a binary vector (0 or 1). For continuous outcomes, it should be a numeric vector.

The function returns a fitted Random Forest model object of class "randomForest".

**Example**:

```{r rf-example}
library(MASS)
data(Boston)
str(Boston) 
X <- Boston[, 2:14]
y <- Boston[,1]
rf_results <- rf_model(X, y) #For Continuous Response variable
rf_results
rf_results$predicted
rf_results$importance

data(Pima.te) 
str(Pima.te)
X <- Pima.te[, 1:7]
y <- Pima.te[,8]
rf_results <- rf_model(X, as.numeric(y) - 1) #For Binary Response variable
rf_results
rf_results$predicted
rf_results$importance
```

### 6. Bagging (`bagged_model`)

**Mathematics**: Bagging involves training multiple models on bootstrapped samples.

**Example**:

```{r bagging-example}
#bagged_results <- bagged_model(X, y, r.bagging = 10, modelType = "linear")
#print(bagged_results)
```

### 7. Ensemble Learning (`ensemble_model`)

Ensemble learning is a powerful technique in machine learning where multiple models are combined to improve overall performance. The intuition behind ensemble learning is that by combining the predictions of multiple models, we can reduce the risk of making a wrong prediction and often achieve better results than any individual model.

Here we are using 2 models as base learners - Elastic Net provides regularization and variable selection capabilities, while Random Forest offers robustness and non-linear modeling capabilities.

Description:
This function accepts two arguments:

X: Matrix of predictors. Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.
y: Response variable. For binary outcomes, it should be a binary vector (0 or 1). For continuous outcomes, it should be a numeric vector.

The function returns a list containing the predictions and accuracies of the individual models and the ensemble model.
ensembled_prediction: The ensemble prediction.
elastic.net: List containing information about the Elastic Net model, including predicted values.
random.forest: List containing information about the Random Forest model, including predicted values.
**Example**:

```{r ensemble-example}
library(MASS)
data(Boston)
str(Boston) 
X <- Boston[, 2:14]
y <- Boston[,1]
ensemble_predictions <- ensemble_predict(X, y) #For Continuous Response variable
ensemble_predictions

data(Pima.te) 
str(Pima.te)
X <- Pima.te[, 1:7]
y <- Pima.te[,8]
ensemble_predictions <- ensemble_predict(X, as.numeric(y) - 1) #For Binary Response variable
ensemble_predictions
```

### 8. Variable Screening (`variable_screening`)

Variable Screening for Top Predictors
This function performs Univariate filtering (Univariate filtering evaluates each feature's individual association with
the target variable through statistical tests, selecting the most significant features based on measures such as correlation, ANOVA, or chi-squared tests.) on each column of the predictor matrix X based on the provided output variable y.
Various statistical tests are applied depending on the data types of X and y to identify significant predictors.
If y is numeric and X is numeric, Pearson's correlation test or Spearman's rank correlation test is applied.
If y is numeric and X is binary, ANOVA or Kendall's rank correlation test is performed.
If y is binary and X is numeric, ANOVA or Fisher's Exact Test is used.
If both y and X are binary, Chi-squared test is applied.

Arguments
X: Matrix of predictors. Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.
y: Response variable. Can be continuous and normally distributed or binary responses.
K: Number of top predictors to select. If not specified, default is half the number of original number of predictor variables.

This returns a dataframe containing the selected predictor variables based on statistical significance.

**Example**:

```{r variable-screening-example}
library(MASS)
data(survey)
str(survey)
X <- survey[, -10] 
y <- survey$Height
selected_variables <- variable_pre_screening(X, y, 4) #For Continuous Response variable
print(selected_variables)
```
### 9. Combining Ensemble, Bagging, Models and Feature Selection (`simple_ensemble_group_22`)

The simple_ensemble_group_22 function allows users to specify a set of models to include in the ensemble and to give option for top K number of predictors to be selected for the model (By default, if n<20*p, where n=sample size and p=number of explanatory variables, it selects for some top predictors cross a certain threshold value for their univariate pValue), allows users to give the number of times to sample to perform bagging for each model, and whether to perform ensemble learning of the models.

Function Arguments
X: Matrix of predictors. Each row represents an observation, and each column represents a predictor. It can be a mix of continuous, discrete, and binary predictors.
y: Response variable. Can be continuous and normally distributed or binary responses.
models: A character vector specifying the types of models to include in the ensemble. Options include "elastic_net" and "random_forest". Default is "elastic_net".
k: Top number of predictors to select in the pre-screening step. Default is NULL.
r.bagging: Number of times model will perform sampling with replacement for the samples. Default is 50.
is.ensemble: Logical parameter indicating whether to perform ensemble learning of models passed as a parameter. Default is FALSE.
```{r simple_ensemble_group_22-example}
library(MASS)
data(Boston)
str(Boston)
X <- Boston[, 2:14]
y <- Boston[,1]
results <- simple_ensemble_group_22(X, y, c("elastic_net", "random_forest"), 50, TRUE) #For Continuous Response variable
print(results)
```
## Conclusion

The `simpleEnsembleGroup22` package provides tools for robust machine learning applications, integrating advanced regression and ensemble techniques to tackle complex predictive modeling tasks.
